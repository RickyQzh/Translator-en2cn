{
    "batch_size": 256,
    "epochs": 30,
    "learning_rate": 0.001,
    "d_model": 256,
    "num_heads": 8,
    "num_encoder_layers": 6,
    "num_decoder_layers": 6,
    "d_ff": 1024,
    "dropout": 0.2,
    "label_smoothing": 0.1,
    "patience": 5,
    "use_positional_encoding": true,
    "use_multihead_attention": true,
    "activation": "relu",
    "position_encoding_type": "sinusoidal",
    "norm_position": "post",
    "use_layer_norm": false,
    "optimizer": "adam",
    "ablation_keys": [
        "use_layer_norm"
    ]
}